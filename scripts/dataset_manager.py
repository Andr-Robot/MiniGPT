#!/usr/bin/env python3
"""
MiniGPT æ•°æ®é›†ç®¡ç†è„šæœ¬
ç”¨äºæŸ¥çœ‹ã€ä¸‹è½½å’Œç®¡ç†è®­ç»ƒæ•°æ®é›†
"""

import os
import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from data.datasets import DatasetLoader


def list_datasets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
    loader = DatasetLoader()
    datasets = loader.list_available_datasets()
    
    print("ğŸ“š å¯ç”¨æ•°æ®é›†åˆ—è¡¨:")
    print("=" * 80)
    
    for name, config in datasets.items():
        info = loader.get_dataset_info(name)
        status = "âœ… å·²ä¸‹è½½" if info['downloaded'] else "âŒ æœªä¸‹è½½"
        size_info = f"({info['file_size']})" if info['downloaded'] else f"({config['size']})"
        
        print(f"\nğŸ”¹ {name.upper()}")
        print(f"   æè¿°: {config['description']}")
        print(f"   çŠ¶æ€: {status} {size_info}")
        print(f"   ç±»å‹: {config['type']}")
        
        if info['downloaded']:
            print(f"   è·¯å¾„: {info['filepath']}")


def download_dataset(dataset_name: str, force: bool = False):
    """ä¸‹è½½æŒ‡å®šæ•°æ®é›†"""
    loader = DatasetLoader()
    
    if dataset_name not in loader.list_available_datasets():
        print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        return
    
    try:
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_name}")
        filepath = loader.download_dataset(dataset_name, force_download=force)
        print(f"âœ… ä¸‹è½½å®Œæˆ: {filepath}")
        
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        info = loader.get_dataset_info(dataset_name)
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {info['file_size']}")
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")


def preview_dataset(dataset_name: str, num_samples: int = 5):
    """é¢„è§ˆæ•°æ®é›†å†…å®¹"""
    loader = DatasetLoader()
    
    if dataset_name not in loader.list_available_datasets():
        print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        return
    
    try:
        print(f"ğŸ‘€ é¢„è§ˆæ•°æ®é›†: {dataset_name}")
        print("=" * 80)
        
        texts = loader.load_dataset(dataset_name, max_samples=num_samples)
        
        for i, text in enumerate(texts, 1):
            print(f"\nğŸ“„ æ ·æœ¬ {i}:")
            print(f"é•¿åº¦: {len(text)} å­—ç¬¦")
            print(f"å†…å®¹: {text[:200]}{'...' if len(text) > 200 else ''}")
            print("-" * 40)
        
        print(f"\nğŸ“Š æ€»æ ·æœ¬æ•°: {len(texts)}")
        
    except Exception as e:
        print(f"âŒ é¢„è§ˆå¤±è´¥: {e}")


def test_dataset_loading(dataset_name: str):
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    loader = DatasetLoader()
    
    if dataset_name not in loader.list_available_datasets():
        print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        return
    
    try:
        print(f"ğŸ§ª æµ‹è¯•æ•°æ®é›†åŠ è½½: {dataset_name}")
        print("=" * 80)
        
        # åŠ è½½æ•°æ®é›†
        texts = loader.load_dataset(dataset_name)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chars = sum(len(text) for text in texts)
        avg_length = total_chars / len(texts) if texts else 0
        
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   æ ·æœ¬æ•°é‡: {len(texts):,}")
        print(f"   æ€»å­—ç¬¦æ•°: {total_chars:,}")
        print(f"   å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
        print(f"   æœ€çŸ­æ ·æœ¬: {min(len(text) for text in texts)} å­—ç¬¦")
        print(f"   æœ€é•¿æ ·æœ¬: {max(len(text) for text in texts)} å­—ç¬¦")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„é¢„è§ˆ
        print(f"\nğŸ“„ å‰3ä¸ªæ ·æœ¬é¢„è§ˆ:")
        for i, text in enumerate(texts[:3], 1):
            print(f"   æ ·æœ¬ {i}: {text[:100]}{'...' if len(text) > 100 else ''}")
        
        print("âœ… æ•°æ®é›†åŠ è½½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


def create_training_config(dataset_name: str):
    """ä¸ºæŒ‡å®šæ•°æ®é›†åˆ›å»ºè®­ç»ƒé…ç½®"""
    loader = DatasetLoader()
    
    if dataset_name not in loader.list_available_datasets():
        print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        return
    
    # æ ¹æ®æ•°æ®é›†ç±»å‹ç”Ÿæˆé…ç½®
    configs = {
        'tiny_shakespeare': {
            'dataset': 'tiny_shakespeare',
            'tokenizer_type': 'char',
            'vocab_size': 100,
            'max_samples': 2000,
            'batch_size': 32,
            'max_length': 128,
            'd_model': 256,
            'n_layers': 6,
            'n_heads': 8,
            'd_ff': 1024,
            'dropout': 0.1,
            'learning_rate': 1e-4,
            'weight_decay': 0.01,
            'device': 'auto'
        },
        'tiny_stories': {
            'dataset': 'tiny_stories',
            'tokenizer_type': 'word',
            'vocab_size': 2000,
            'max_samples': 5000,
            'batch_size': 32,
            'max_length': 128,
            'd_model': 256,
            'n_layers': 6,
            'n_heads': 8,
            'd_ff': 1024,
            'dropout': 0.1,
            'learning_rate': 1e-4,
            'weight_decay': 0.01,
            'device': 'auto'
        },
        'wikitext_103': {
            'dataset': 'wikitext_103',
            'tokenizer_type': 'word',
            'vocab_size': 5000,
            'max_samples': 10000,
            'batch_size': 32,
            'max_length': 128,
            'd_model': 256,
            'n_layers': 6,
            'n_heads': 8,
            'd_ff': 1024,
            'dropout': 0.1,
            'learning_rate': 1e-4,
            'weight_decay': 0.01,
            'device': 'auto'
        },
        'ag_news': {
            'dataset': 'ag_news',
            'tokenizer_type': 'word',
            'vocab_size': 5000,
            'max_samples': 5000,
            'batch_size': 32,
            'max_length': 128,
            'd_model': 256,
            'n_layers': 6,
            'n_heads': 8,
            'd_ff': 1024,
            'dropout': 0.1,
            'learning_rate': 1e-4,
            'weight_decay': 0.01,
            'device': 'auto'
        }
    }
    
    config = configs.get(dataset_name, {})
    
    print(f"âš™ï¸ ä¸ºæ•°æ®é›† {dataset_name} ç”Ÿæˆçš„è®­ç»ƒé…ç½®:")
    print("=" * 80)
    
    import json
    print(json.dumps(config, indent=2, ensure_ascii=False))
    
    # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    config_file = f"config_{dataset_name}.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {config_file}")
    print(f"ğŸš€ ä½¿ç”¨æ–¹æ³•: python train.py --config {config_file}")


def main():
    parser = argparse.ArgumentParser(description='MiniGPT æ•°æ®é›†ç®¡ç†å·¥å…·')
    parser.add_argument('action', choices=['list', 'download', 'preview', 'test', 'config'],
                       help='è¦æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--dataset', type=str, help='æ•°æ®é›†åç§°')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°ä¸‹è½½')
    parser.add_argument('--samples', type=int, default=5, help='é¢„è§ˆæ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    if args.action == 'list':
        list_datasets()
    elif args.action == 'download':
        if not args.dataset:
            print("âŒ è¯·æŒ‡å®šè¦ä¸‹è½½çš„æ•°æ®é›†åç§°")
            return
        download_dataset(args.dataset, args.force)
    elif args.action == 'preview':
        if not args.dataset:
            print("âŒ è¯·æŒ‡å®šè¦é¢„è§ˆçš„æ•°æ®é›†åç§°")
            return
        preview_dataset(args.dataset, args.samples)
    elif args.action == 'test':
        if not args.dataset:
            print("âŒ è¯·æŒ‡å®šè¦æµ‹è¯•çš„æ•°æ®é›†åç§°")
            return
        test_dataset_loading(args.dataset)
    elif args.action == 'config':
        if not args.dataset:
            print("âŒ è¯·æŒ‡å®šè¦ç”Ÿæˆé…ç½®çš„æ•°æ®é›†åç§°")
            return
        create_training_config(args.dataset)


if __name__ == "__main__":
    main() 