import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import time
import os
import json
from typing import Dict, List, Optional, Tuple
import matplotlib.pyplot as plt
from tqdm import tqdm
import sys
import numpy as np
import argparse
import traceback

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from core.mini_gpt import MiniGPT, count_parameters
from data.tokenizer import SimpleTokenizer, CharTokenizer, BPETokenizer
from data.datasets import DatasetLoader
from utils.utils import visualize_attention_weights, plot_parameter_distribution


class Trainer:
    """MiniGPTè®­ç»ƒå™¨"""
    
    def __init__(self, model: MiniGPT, tokenizer, train_dataloader: DataLoader,
                 val_dataloader: Optional[DataLoader] = None,
                 learning_rate: float = 1e-4, weight_decay: float = 0.01,
                 device: str = 'auto'):
        self.model = model
        self.tokenizer = tokenizer
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.device = self._get_device(device)
        
        # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
        self.model.to(self.device)
        
        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=len(train_dataloader)
        )
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.val_losses = []
        self.learning_rates = []
        
    def _get_device(self, device: str) -> torch.device:
        """è·å–è®­ç»ƒè®¾å¤‡"""
        if device == 'auto':
            return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        return torch.device(device)
    
    def train_epoch(self) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(self.train_dataloader)
        
        progress_bar = tqdm(self.train_dataloader, desc="è®­ç»ƒä¸­")
        
        for batch_idx, (input_ids, target_ids) in enumerate(progress_bar):
            # ç§»åˆ°è®¾å¤‡
            input_ids = input_ids.to(self.device)
            target_ids = target_ids.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            logits, loss = self.model(input_ids, target_ids)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # ä¼˜åŒ–å™¨æ­¥è¿›
            self.optimizer.step()
            self.scheduler.step()
            
            # è®°å½•æŸå¤±
            total_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{self.scheduler.get_last_lr()[0]:.6f}'
            })
            
            # è®°å½•å­¦ä¹ ç‡
            self.learning_rates.append(self.scheduler.get_last_lr()[0])
        
        return total_loss / num_batches
    
    def validate(self) -> float:
        """éªŒè¯æ¨¡å‹"""
        if self.val_dataloader is None:
            return 0.0
            
        self.model.eval()
        total_loss = 0.0
        num_batches = len(self.val_dataloader)
        
        with torch.no_grad():
            for input_ids, target_ids in tqdm(self.val_dataloader, desc="éªŒè¯ä¸­"):
                input_ids = input_ids.to(self.device)
                target_ids = target_ids.to(self.device)
                
                logits, loss = self.model(input_ids, target_ids)
                total_loss += loss.item()
        
        return total_loss / num_batches
    
    def train(self, num_epochs: int, save_dir: str = 'output/checkpoints',
              save_every: int = 5, early_stopping_patience: int = 10) -> Dict:
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        print(f"å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {count_parameters(self.model):,}")
        print(f"è®­ç»ƒæ ·æœ¬æ•°é‡: {len(self.train_dataloader.dataset)}")
        if self.val_dataloader:
            print(f"éªŒè¯æ ·æœ¬æ•°é‡: {len(self.val_dataloader.dataset)}")
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch + 1}/{num_epochs}")
            print("-" * 50)
            
            # è®­ç»ƒ
            start_time = time.time()
            train_loss = self.train_epoch()
            train_time = time.time() - start_time
            
            # éªŒè¯
            val_loss = self.validate()
            
            # è®°å½•å†å²
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            # æ‰“å°ç»“æœ
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"è®­ç»ƒæ—¶é—´: {train_time:.2f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self.save_model(os.path.join(save_dir, 'best_model.pth'))
                print("ä¿å­˜æœ€ä½³æ¨¡å‹")
            else:
                patience_counter += 1
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % save_every == 0:
                checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch + 1}.pth')
                self.save_model(checkpoint_path)
                print(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
            
            # æ—©åœ
            if patience_counter >= early_stopping_patience:
                print(f"æ—©åœè§¦å‘ï¼Œ{early_stopping_patience}ä¸ªepochæ²¡æœ‰æ”¹å–„")
                break
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history(save_dir)
        
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates,
            'best_val_loss': best_val_loss
        }
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'tokenizer': self.tokenizer,
            'model_config': {
                'vocab_size': self.model.vocab_size,
                'd_model': self.model.d_model,
                'n_layers': len(self.model.transformer_blocks),
                'n_heads': self.model.transformer_blocks[0].attention.n_heads,
                'd_ff': self.model.transformer_blocks[0].feed_forward.linear1.out_features,
                'max_len': self.model.position_encoding.pe.size(0),
                'dropout': self.model.dropout.p
            }
        }, filepath)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.tokenizer = checkpoint['tokenizer']
        print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")
    
    def save_training_history(self, save_dir: str):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates
        }
        
        with open(os.path.join(save_dir, 'training_history.json'), 'w') as f:
            json.dump(history, f, indent=2)
    
    def plot_training_history(self, save_dir: str = 'output/plots'):
        """Plot training history (all labels/titles in English)"""
        os.makedirs(save_dir, exist_ok=True)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss curve
        ax1.plot(self.train_losses, label='Train Loss', color='blue')
        if self.val_losses:
            ax1.plot(self.val_losses, label='Validation Loss', color='red')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.legend()
        ax1.grid(True)
        
        # Learning rate curve
        ax2.plot(self.learning_rates, color='green')
        ax2.set_xlabel('Step')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('Learning Rate Schedule')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'training_history.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_sample(self, prompt: str, max_length: int = 50, 
                       temperature: float = 1.0, top_k: int = 50) -> str:
        """ç”Ÿæˆç¤ºä¾‹æ–‡æœ¬"""
        self.model.eval()
        
        # ç¼–ç æç¤ºï¼ˆä¸æ·»åŠ EOS tokenï¼‰
        prompt_ids = self.tokenizer.encode(prompt, add_eos=False)
        prompt_tensor = torch.tensor([prompt_ids], dtype=torch.long).to(self.device)
        
        # è®¡ç®—éœ€è¦ç”Ÿæˆçš„æ–°tokenæ•°é‡
        tokens_to_generate = max_length - len(prompt_ids)
        
        if tokens_to_generate <= 0:
            # å¢åŠ ç”Ÿæˆé•¿åº¦
            max_length = len(prompt_ids) + 20
            tokens_to_generate = 20
        
        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.model.generate(
                prompt_tensor, 
                max_length=max_length,
                temperature=temperature,
                top_k=top_k
            )
        
        # è§£ç 
        generated_text = self.tokenizer.decode(generated_ids[0].tolist())
        
        return generated_text


def create_trainer_from_config(config: Dict) -> Trainer:
    """ä»é…ç½®åˆ›å»ºè®­ç»ƒå™¨"""
    
    # åˆ›å»ºåˆ†è¯å™¨
    if config['tokenizer_type'] == 'word':
        tokenizer = SimpleTokenizer(vocab_size=config['vocab_size'])
    elif config['tokenizer_type'] == 'char':
        tokenizer = CharTokenizer(vocab_size=config['vocab_size'])
    elif config['tokenizer_type'] == 'bpe':
        tokenizer = BPETokenizer(vocab_size=config['vocab_size'])
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„åˆ†è¯å™¨ç±»å‹: {config['tokenizer_type']}")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset_name = config.get('dataset', 'ag_news')  # é»˜è®¤ä½¿ç”¨AG Newsæ•°æ®é›†
    
    # ä½¿ç”¨DatasetLoaderåŠ è½½çœŸå®æ•°æ®é›†
    loader = DatasetLoader()
    
    if dataset_name == 'curated':
        # ä½¿ç”¨ç²¾é€‰æ•°æ®é›†ï¼ˆæ··åˆå¤šä¸ªæ•°æ®é›†ï¼‰
        from data.datasets import create_curated_dataset
        all_texts = create_curated_dataset()
    elif dataset_name in ['tiny_shakespeare', 'tiny_stories', 'ag_news']:
        # ä½¿ç”¨å•ä¸ªçœŸå®æ•°æ®é›†
        all_texts = loader.load_dataset(dataset_name, config.get('max_samples', None))
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    split_idx = int(len(all_texts) * 0.8)
    train_texts = all_texts[:split_idx]
    val_texts = all_texts[split_idx:]
    
    # è®­ç»ƒåˆ†è¯å™¨
    tokenizer.fit(train_texts + val_texts)
    
    # åˆ›å»ºç®€å•çš„æ•°æ®åŠ è½½å™¨
    from torch.utils.data import Dataset, DataLoader
    
    class SimpleTextDataset(Dataset):
        def __init__(self, texts, tokenizer, max_length):
            self.texts = texts
            self.tokenizer = tokenizer
            self.max_length = max_length
            
            # ç¼–ç æ‰€æœ‰æ–‡æœ¬
            self.encoded_texts = []
            for text in texts:
                encoded = tokenizer.encode(text, max_length=max_length)
                if len(encoded) > 2:  # è‡³å°‘è¦æœ‰å¼€å§‹ã€ç»“æŸå’Œå†…å®¹token
                    self.encoded_texts.append(encoded)
        
        def __len__(self):
            return len(self.encoded_texts)
        
        def __getitem__(self, idx):
            encoded = self.encoded_texts[idx]
            # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡ï¼ˆç”¨äºè¯­è¨€æ¨¡å‹è®­ç»ƒï¼‰
            input_ids = encoded[:-1]
            target_ids = encoded[1:]
            return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = SimpleTextDataset(train_texts, tokenizer, config['max_length'])
    val_dataset = SimpleTextDataset(val_texts, tokenizer, config['max_length'])
    
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        drop_last=True
    )
    
    val_dataloader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        drop_last=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    vocab_size = len(tokenizer.word2idx) if hasattr(tokenizer, 'word2idx') else len(tokenizer.char2idx)
    model = MiniGPT(
        vocab_size=vocab_size,
        d_model=config['d_model'],
        n_layers=config['n_layers'],
        n_heads=config['n_heads'],
        d_ff=config['d_ff'],
        max_len=config['max_length'],
        dropout=config['dropout']
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        learning_rate=config['learning_rate'],
        weight_decay=config['weight_decay'],
        device=config['device']
    )
    
    return trainer


def load_config(config_name: str) -> Dict:
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®
    
    Args:
        config_name: é…ç½®æ–‡ä»¶åï¼ˆä¸åŒ…å«.jsonæ‰©å±•åï¼‰
        
    Returns:
        é…ç½®å­—å…¸
    """
    config_path = os.path.join(project_root, 'configs', f'{config_name}.json')
    
    if os.path.exists(config_path):
        print(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½: {config_path}")
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        # åˆå¹¶æ¨¡å‹é…ç½®å’Œè®­ç»ƒé…ç½®
        config = {
            **config_data['model_config'],
            **config_data['training_config']
        }
        
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: {config_data['name']}")
        print(f"ğŸ“ æè¿°: {config_data['description']}")
        print(f"ğŸ’¡ ç”¨é€”: {config_data['use_case']}")
        print(f"ğŸ“Š é¢„ä¼°å‚æ•°æ•°é‡: {config_data['estimated_params']:,}")
        
        return config
    else:
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("ğŸ”§ ä½¿ç”¨é»˜è®¤é…ç½®...")
        return None


def get_available_configs() -> List[str]:
    """è·å–å¯ç”¨çš„é…ç½®æ–‡ä»¶åˆ—è¡¨"""
    config_dir = os.path.join(project_root, 'configs')
    if not os.path.exists(config_dir):
        return []
    
    configs = []
    for file in os.listdir(config_dir):
        if file.endswith('.json'):
            configs.append(file[:-5])  # ç§»é™¤.jsonæ‰©å±•å
    
    return sorted(configs)


def get_available_datasets() -> List[str]:
    """è·å–å¯ç”¨çš„çœŸå®å¼€æºæ•°æ®é›†åˆ—è¡¨"""
    return [
        'ag_news',          # AG Newsæ–°é—»æ•°æ®é›†
        'tiny_shakespeare', # èå£«æ¯”äºšæ–‡æœ¬
        'tiny_stories',     # å°æ•…äº‹æ•°æ®é›†
        'curated'           # ç²¾é€‰æ··åˆæ•°æ®é›†
    ]


def train_model(config: Dict, config_name: str):
    """
    ä½¿ç”¨ç»™å®šé…ç½®è®­ç»ƒæ¨¡å‹
    
    Args:
        config: è®­ç»ƒé…ç½®
        config_name: é…ç½®åç§°
    """
    print(f"\nğŸš€ MiniGPTè®­ç»ƒé…ç½®: {config_name}")
    print("=" * 60)
    print(json.dumps(config, indent=2, ensure_ascii=False))
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = create_trainer_from_config(config)
    
    # è·å–è®­ç»ƒå‚æ•°
    num_epochs = config.get('num_epochs', 10)
    save_every = config.get('save_every', 2)
    early_stopping_patience = config.get('early_stopping_patience', 3)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"ğŸ’¾ ä¿å­˜é¢‘ç‡: æ¯{save_every}è½®")
    print(f"â¹ï¸  æ—©åœè€å¿ƒå€¼: {early_stopping_patience}")
    
    history = trainer.train(
        num_epochs=num_epochs,
        save_dir='output/checkpoints',
        save_every=save_every,
        early_stopping_patience=early_stopping_patience
    )
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    trainer.plot_training_history()
    
    # ç”Ÿæˆç¤ºä¾‹
    print("\n" + "="*50)
    print("ç”Ÿæˆç¤ºä¾‹æµ‹è¯•")
    print("="*50)

    # é’ˆå¯¹ä¸åŒæ•°æ®é›†é€‰æ‹©ä¸åŒçš„æµ‹è¯•prompt
    dataset_name = config.get('dataset', 'ag_news')
    if dataset_name == 'ag_news':
        prompts = [
            "The president announced",
            "Stock market rises",
            "Scientists discovered",
            "In a recent study",
            "Technology news"
        ]
    elif dataset_name == 'tiny_shakespeare':
        prompts = [
            "To be, or not to be",
            "My lord",
            "What say you",
            "love and honor",
            "O Romeo"
        ]
    elif dataset_name == 'tiny_stories':
        prompts = [
            "Once upon a time",
            "There was a little cat",
            "The boy found a magic stone",
            "In a small village",
            "The adventure begins"
        ]
    elif dataset_name == 'curated':
        prompts = [
            "To be, or not to be",
            "The president announced",
            "Once upon a time",
            "Technology news",
            "love and honor"
        ]
    else:
        prompts = [
            "world",
            "machine learning",
            "artificial intelligence",
            "technology",
            "business"
        ]

    for prompt in prompts:
        print(f"\n--- æµ‹è¯•æç¤º: '{prompt}' ---")
        try:
            prompt_ids = trainer.tokenizer.encode(prompt, add_eos=False)
            if 1 in prompt_ids:
                print("âš ï¸  è­¦å‘Šï¼šæç¤ºè¯ä¸­åŒ…å« <UNK>ï¼Œå»ºè®®æ¢è¯æˆ–å¢å¤§è¯è¡¨ï¼")
                print(f"   ç¼–ç ç»“æœ: {prompt_ids}")

            generated = trainer.generate_sample(
                prompt, 
                max_length=len(prompt_ids) + 20,
                temperature=0.8,
                top_k=20
            )
            print(f"æœ€ç»ˆç»“æœ: '{generated}'")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        print("-" * 50)


def main():
    parser = argparse.ArgumentParser(description='MiniGPT è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, default='small', 
                       help='é…ç½®æ–‡ä»¶åç§° (small/medium/large)')
    parser.add_argument('--dataset', type=str, default='sample',
                       help='æ•°æ®é›†åç§° (sample/ag_news/tiny_shakespeare/tiny_stories/curated)')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œå¿½ç•¥é…ç½®æ–‡ä»¶')
    parser.add_argument('--help-configs', action='store_true',
                       help='æ˜¾ç¤ºå¯ç”¨çš„é…ç½®æ–‡ä»¶')
    parser.add_argument('--help-datasets', action='store_true',
                       help='æ˜¾ç¤ºå¯ç”¨çš„æ•°æ®é›†')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if args.help_configs:
        print("ğŸ“‹ å¯ç”¨çš„é…ç½®æ–‡ä»¶:")
        configs = get_available_configs()
        for config in configs:
            print(f"   - {config}")
        return
    
    if args.help_datasets:
        print("ğŸ“‹ å¯ç”¨çš„æ•°æ®é›†:")
        datasets = get_available_datasets()
        for dataset in datasets:
            print(f"   - {dataset}")
        return
    
    # éªŒè¯æ•°æ®é›†
    available_datasets = get_available_datasets()
    if args.dataset not in available_datasets:
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†: {args.dataset}")
        print("ğŸ“‹ å¯ç”¨çš„æ•°æ®é›†:")
        for dataset in available_datasets:
            print(f"   - {dataset}")
        exit(1)
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ MiniGPT")
    print(f"ğŸ“Š æ•°æ®é›†: {args.dataset}")
    print(f"âš™ï¸  é…ç½®: {args.config}")
    print("=" * 50)
    
    # åŠ è½½é…ç½®
    if not args.force:
        config = load_config(args.config)
        if config is None:
            print("âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥")
            exit(1)
    else:
        # å¼ºåˆ¶ä½¿ç”¨é»˜è®¤é…ç½®
        print("ğŸ”§ ä½¿ç”¨é»˜è®¤é…ç½®...")
        configs = get_available_configs()
        if args.config in configs:
            config = load_config(args.config)
            if config is None:
                print("âŒ é»˜è®¤é…ç½®åŠ è½½å¤±è´¥")
                exit(1)
        else:
            print(f"âŒ æœªæ‰¾åˆ°é»˜è®¤é…ç½®: {args.config}")
            print("ğŸ“‹ å¯ç”¨çš„é…ç½®:")
            for name in configs:
                print(f"   - {name}")
            exit(1)
    
    # æ·»åŠ æ•°æ®é›†ä¿¡æ¯åˆ°é…ç½®ä¸­
    config['dataset'] = args.dataset
    config_name = f"{args.config} + {args.dataset}"
    
    # å¼€å§‹è®­ç»ƒ
    try:
        train_model(config, config_name)
        print("âœ… è®­ç»ƒå®Œæˆ!")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main() 